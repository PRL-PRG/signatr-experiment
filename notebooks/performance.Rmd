---
title: "Performance analysis of the pipeline for the spxdb database"
output: html_notebook
---


```{r setup}
library(targets)
library(dplyr)
library(ggplot2)
library(sxpdb)

knitr::opts_knit$set(root.dir = normalizePath("..")) 
```

# Performance of tracing 

```{r durations}
traced_durations <- tar_meta(fields = "seconds", complete_only = TRUE, names = starts_with("traced_results")) %>% filter(name %in% names(tar_read(traced_results)))
normal_durations <- tar_meta(fields = "seconds", complete_only = TRUE, names = starts_with("run_results2")) %>% filter(name %in% tar_branches(run_results2, map(individual_files))$run_results2)
```

```{r slowdown}
slowdown <- tibble(slow = traced_durations$seconds / normal_durations$seconds)
```

The average of the slowdowns is `r mean(slowdown$slow)` and the median is `r median(slowdown$slow)`, with a standard deviation of `r sd(slowdown$slow)`.

```{r slowdown-plot}
slowdown %>% ggplot(aes(x = slow)) + geom_histogram(bins = 100)
```

What are the outliers?

```{r slowdown-outliers}
outliers <- slowdown %>% filter(slow > 20)
```


# Estimating the setup slowdown

We know the following values:

- the running time without any tracing and recording per file $T_n$
- the running time with tracing and recording per file $T_t$
- the number of values, their size $\text{size}(v)$, and the number of times we see a value $k_v$ , per file

```{r known-values}
db_paths <- list.dirs("data/sxpdb", recursive =  FALSE) %>% purrr::discard(function(p) endsWith(p, "cran_db"))

l <- list()

i <- 1
for(db_path in db_paths) {
  db <- open_db(db_path)
  
  meta <- view_meta_db(db)
    
  l[[i]] <- tibble::tibble_row(db_path = db_path, size = sum(meta$size), n_values = size_db(db))
  
  close_db(db)
  
  i <- i + 1
}

sizes_db <- bind_rows(l)
```


```{r}
diff_traced_normal <- traced_durations %>% mutate(diff_sec = traced_durations$seconds - normal_durations$seconds)

file_id <- tar_branches(traced_results, individual_files) %>% mutate(file_path = tar_read(individual_files), db_path = file.path("data", "sxpdb", basename(file_path)))

diff_traced_normal <- diff_traced_normal %>% left_join(file_id, by=c("name" = "traced_results")) %>% select(db_path, diff_sec)

df <- diff_traced_normal %>% left_join(sizes_db, by = "db_path")
```

```{r setup-slowdown-plot}
ggplot(df, aes(x = size, y = diff_sec,colour = n_values)) + geom_point() + geom_smooth(method = "glm") 
```

And the model:

```{r}
mod_df <- glm(diff_sec ~ size, data = df)
```

So the startup slowdown in seconds is: `r mod_df$coefficients[[1]]`


# Recording performance

Recording a new value involves:

- serializing it
- hashing it (SHA1)
- writing it to the disk if it has not been written yet

The SEXP address optimization can also speed up the process during one run: we create a SEXP address -> hashed serialized SEXP mapping (that just needs to be updated when a new value is allocated at a garbage collected SEXP address).

During one run, we record:

- the first seen duration
- the next seen durations (as an average)
- the number of times we have seen the value

We also record the number of times a value has been seen when merging databases.
